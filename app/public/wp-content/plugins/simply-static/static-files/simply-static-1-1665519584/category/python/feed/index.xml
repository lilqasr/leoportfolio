<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>Python &#8211; leo-portfolio</title>
	<atom:link href="https://lilqasr.github.io/leoportfolio/category/python/feed/" rel="self" type="application/rss+xml" />
	<link>https://lilqasr.github.io/leoportfolio/</link>
	<description>Just another WordPress site</description>
	<lastBuildDate>Sun, 09 Oct 2022 13:37:37 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=6.0.2</generator>
	<item>
		<title>Building a dataset from published pdf files</title>
		<link>https://lilqasr.github.io/leoportfolio/building-a-dataset-from-published-pdf-files/</link>
					<comments>https://lilqasr.github.io/leoportfolio/building-a-dataset-from-published-pdf-files/#respond</comments>
		
		<dc:creator><![CDATA[leolab88]]></dc:creator>
		<pubDate>Sun, 09 Oct 2022 13:36:46 +0000</pubDate>
				<category><![CDATA[Python]]></category>
		<category><![CDATA[Build]]></category>
		<category><![CDATA[Clean]]></category>
		<guid isPermaLink="false">https://lilqasr.github.io/leoportfolio/?p=68</guid>

					<description><![CDATA[As i show you in a last post, i used python with BeautifulSoup to download several of pdf files published in a government institution in order to make an analysis of this information. The scraping was the first part of the work, but then it was necessary to build the database. To do that i used, again python, using its Pandas library. Here is the [&#8230;]]]></description>
										<content:encoded><![CDATA[
<p>As i show you in a last post, i used python with BeautifulSoup to download several of pdf files published in a government institution in order to make an analysis of this information. The scraping was the first part of the work, but then it was necessary to build the database. </p>



<p>To do that i used, again python, using its Pandas library. Here is the notebook:</p>



<!--–– replace src and id. Note that the id appears in two places ––-->
<iframe src="https://lilqasr.github.io/leoportfolio/wp-content/uploads/2022/10/Resoluciones_auditorias_publicadas-2015-2020-part2.html" id="DatasetPDF_published_CGR" scrolling="no" width="100%" frameborder="0" style="border: 1px lightgrey solid;">
</iframe>
<script>
(function(){
    var iframe = document.getElementById("DatasetPDF_published_CGR");
    function resize() {
        iframe.style.height = 
        (iframe.contentWindow.document.body.scrollHeight + 4) + 'px';
        // "body" in the above line may need to be changed,
        // depending on the specific version of Jupyter.
        // For example, it may have to be replaced with 
        // .getElementById("notebook-container"), and the 
        // DOM and the id may need to be manually added.
    }
    window.addEventListener("resize", resize);
    iframe.addEventListener("load", resize);
})()
</script>



<p>After building the dataset, and clean the first part, it was necessary to make a new job, that was check the similarity values in the &#8220;institucion&#8221; column. As was shown, this column has a lot of problems because some names are repeated with misspelling.</p>



<p>A time ago, i found out that there is a tool develop by the open source community called <a rel="noreferrer noopener" href="https://openrefine.org/" target="_blank">OpenRefine</a> and help to clean the messy data in a dataset. But in my case i wanted to do something with python and i also find out that there was a library called FuzzyWuzzy, that works similarly as OpenRefine. </p>



<p>What i understood from FuzzyWuzzy was that you need to have a list, series or whatever you want of the correct names that you want to change. Since a did not have this, i had to build it. And below is the work:</p>



<!--–– replace src and id. Note that the id appears in two places ––-->
<iframe src="https://lilqasr.github.io/leoportfolio/wp-content/uploads/2022/10/Limpieza_Auditorias_publicadas_2015-2020_sitioCGR.html" id="Limpieza_DatasetPDF_published_CGR" scrolling="no" width="100%" frameborder="0" style="border: 1px lightgrey solid;">
</iframe>
<script>
(function(){
    var iframe = document.getElementById("Limpieza_DatasetPDF_published_CGR");
    function resize() {
        iframe.style.height = 
        (iframe.contentWindow.document.body.scrollHeight + 4) + 'px';
        // "body" in the above line may need to be changed,
        // depending on the specific version of Jupyter.
        // For example, it may have to be replaced with 
        // .getElementById("notebook-container"), and the 
        // DOM and the id may need to be manually added.
    }
    window.addEventListener("resize", resize);
    iframe.addEventListener("load", resize);
})()
</script>



<p>The last phase of this project would be the analysis of the dataset, but this i will show you in this next post.</p>



<p></p>
]]></content:encoded>
					
					<wfw:commentRss>https://lilqasr.github.io/leoportfolio/building-a-dataset-from-published-pdf-files/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>Download PDF files from a website</title>
		<link>https://lilqasr.github.io/leoportfolio/extract-pdf-website-government/</link>
					<comments>https://lilqasr.github.io/leoportfolio/extract-pdf-website-government/#respond</comments>
		
		<dc:creator><![CDATA[leolab88]]></dc:creator>
		<pubDate>Wed, 28 Sep 2022 17:57:12 +0000</pubDate>
				<category><![CDATA[Python]]></category>
		<category><![CDATA[Scraping]]></category>
		<guid isPermaLink="false">https://lilqasr.github.io/leoportfolio/?p=43</guid>

					<description><![CDATA[Even though, i'm not a data scientist, i have experience building datasets, most of them from public sector institutions of my country (Nicaragua), but when i found that the webscrape existed, i tried to learn it, and was a relief.]]></description>
										<content:encoded><![CDATA[
<p>Even though, i&#8217;m not a data scientist, i have experience building datasets, most of them from public sector institutions of my country (Nicaragua). Since i hadn&#8217;t the knowledge when i had to do researches to get official documentation that is published on public institutions websites, was very difficult. </p>



<p>The public sector in Nicaragua is the second most corrupt and one of the least transparent in Latinamerica. It does not take the most of the advantages offered by telecommunication technologies. But for some years, some of its institutions are publishing lot of information, but not in a simple way.</p>



<p>So, i was asked to make a report about the situation of the accountability in local governments in Nicaragua. I tried to adapt a methodology from the International Budget Partnership  to develop its Open Budget Survey, and one of the task in order to do that was check how many Audit Reports are published on the Contraloría General de la República (CGR) website. </p>



<p>The first thing was to get a dataset with the data published within the Governance Report of CGR between 2011 and 2020. Then, looking into that information, i wanted to know if they published all the audits report they say they do every year, which leaded me to download all the information in those section of the website. </p>



<p>I did not mention before, but this website only works with visitor from Nicaragua&#8217;s IP. In order to access it, i had to use a VPN with a Nicaragua IP, and when i was running the scraper code i had to activate it.</p>



<h2>Web Scraping with python</h2>



<p>Since i recently started using Python, i&#8217;m familiar with the language structured and i found out that has powerful tools to make the web scraping. </p>



<p>So, after a lot of research, i found that one of the best tools for web scraping is the Beautiful Sup library. This, combining others python&#8217;s modules as &#8220;requests&#8221; and &#8220;urllib.parse&#8221;, was going to give me the results i wanted.</p>



<p>The place where is located the information i wanted to substract is in the document repository (Repositorio de documentos) section. Then it is necessary to access inside the &#8220;Auditorías Aprobadas por el Consejo Superior. Then, the information is extracted by year and Audit Unit: &#8220;CGR_Fondos_Propios&#8221;, &#8220;FirmasPrivadas&#8221;, &#8220;UAI&#8221;  . </p>



<p><strong>Here you have my jupyter notebook with part of the work:</strong></p>



<!--–– replace src and id. Note that the id appears in two places ––-->
<iframe src="https://lilqasr.github.io/leoportfolio/wp-content/uploads/2022/10/CGR_PDF-FILES_DOWNLOAD.html" id="webscrapePDF_CGR" scrolling="no" width="100%" frameborder="0" style="border: 1px lightgrey solid;">
</iframe>
<script>
(function(){
    var iframe = document.getElementById("webscrapePDF_CGR");
    function resize() {
        iframe.style.height = 
        (iframe.contentWindow.document.body.scrollHeight + 4) + 'px';
        // "body" in the above line may need to be changed,
        // depending on the specific version of Jupyter.
        // For example, it may have to be replaced with 
        // .getElementById("notebook-container"), and the 
        // DOM and the id may need to be manually added.
    }
    window.addEventListener("resize", resize);
    iframe.addEventListener("load", resize);
})()
</script>



<p>After done this part, it was necessary to merge all of the tables and get information of how many Audits Resolutions Reports were published by the CGR each year and by unit audit.</p>



<p>In conclusion, with these few codes, using BeautifulSoup, you can download a files that you&#8217;re interested in and also get a dataset of the information in order to make the analysis you want. But that i will show you in another post.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://lilqasr.github.io/leoportfolio/extract-pdf-website-government/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
	</channel>
</rss>
